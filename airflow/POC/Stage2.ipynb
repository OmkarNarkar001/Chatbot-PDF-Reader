{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the title, image, summary, and file for each PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from uuid import uuid4\n",
    "from selenium import webdriver\n",
    "from dotenv import load_dotenv\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filepath):\n",
    "    \"\"\"Helper function to download a file from a given URL.\"\"\"\n",
    "   \n",
    "    response = requests.get(url, stream = True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size = 8192):\n",
    "                f.write(chunk)\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(title, url):\n",
    "\n",
    "    status = False\n",
    "\n",
    "    try:\n",
    "        # We need Javascript rendering to see the content\n",
    "        # Ask Selenium to use Google Chrome as the driver\n",
    "\n",
    "        # Set WebDriver options (headless mode to run without UI)\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        \n",
    "        # Ensure you download the right chrome driver from the URL mentioned above for your OS\n",
    "        # Create a folder named 'chromedriver' and store the chromedriver in it \n",
    "        \n",
    "        # chromedriver.exe will work only on Windows\n",
    "        chromedriver_directory = os.path.join(os.path.dirname(os.getcwd()), \"chromedriver\", \"chromedriver.exe\")\n",
    "        service = Service(chromedriver_directory)\n",
    "\n",
    "        # Load the driver from Google Chrome\n",
    "        driver = webdriver.Chrome(options = options, service = service)\n",
    "\n",
    "        # Make a GET request\n",
    "        driver.get(url)\n",
    "\n",
    "        # Sleep for 10 seconds to ensure the page is completely loaded before proceeding\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Fetch the source code of the webpage so we can process it\n",
    "        page_source_code = driver.page_source\n",
    "\n",
    "        # Call BeautifulSoup to parse the HTML content\n",
    "        soup = BeautifulSoup(page_source_code, \"html.parser\")\n",
    "\n",
    "        # Domain prefix for URLs\n",
    "        url_prefix = \"https://rpc.cfainstitute.org\"\n",
    "\n",
    "        # Extract the PDF download url\n",
    "        download_content = soup.find('a', class_=\"content-asset--primary\")\n",
    "        download_url = \"\"\n",
    "        if download_content:\n",
    "            download_url = url_prefix + download_content.get(\"href\", \"\")\n",
    "\n",
    "        # Extract the book cover image\n",
    "        cover_image_content = soup.find('img', class_=\"article-cover\")\n",
    "        cover_image_url = \"\"\n",
    "        if cover_image_content:\n",
    "            cover_image_url = url_prefix + cover_image_content.get(\"src\", \"\").split('?')[0]\n",
    "        \n",
    "        # Extract the overview (which will be used as summary)\n",
    "        # overview = \"\"\n",
    "        # overview_content = soup.find_all('div', class_='article__paragraph')\n",
    "\n",
    "        # if overview_content:\n",
    "        #     for div in overview_content:\n",
    "        #         paragraphs = div.find_all('p')\n",
    "        #         for para in paragraphs:\n",
    "        #             overview += unidecode(str(para.get_text()).strip().replace(\"\\n\", \"\"))\n",
    "\n",
    "        overview = \"\"\n",
    "\n",
    "        # If <div class=\"article__paragraph\"> is available, scrape the text from it\n",
    "        overview_content = soup.find_all('div', class_='article__paragraph')\n",
    "        if overview_content:\n",
    "            for div in overview_content:\n",
    "                \n",
    "                # Extract text from <p>, <ol>, and <ul> tags\n",
    "                for tag in div.find_all(['p', 'ol', 'ul']):\n",
    "                    overview = overview + \" \" + unidecode(tag.get_text().strip().replace(\"\\n\", \"\"))\n",
    "\n",
    "        # Fallback if <div class=\"article__paragraph\"> is not found (Aggressive scraping)\n",
    "        if not overview:\n",
    "            article_body = soup.find('article', class_='grid__item--article-body')\n",
    "            if article_body:\n",
    "                \n",
    "                # Extract from <span class=\"overview__content\">\n",
    "                span_content = article_body.find('span', class_='overview__content')\n",
    "                if span_content:\n",
    "                    for para in span_content.find_all('p'):\n",
    "                        overview = overview + \" \" + unidecode(para.get_text().strip().replace(\"\\n\", \"\"))\n",
    "\n",
    "                # Extract from <div> tags without any class\n",
    "                div_without_class = article_body.find_all('div', class_=None)\n",
    "                for div in div_without_class:\n",
    "                    \n",
    "                    for tag in div.find_all(['p', 'ol', 'ul'], class_=None):\n",
    "                        overview = overview + \" \" + unidecode(tag.get_text().strip().replace(\"\\n\", \"\").replace(\"\\t\\t\\t\\t\\t\", \" \"))\n",
    "\n",
    "        # Save all downloads to download directory\n",
    "        download_dir = os.getenv(\"DOWNLOAD_DIRECTORY\", None)\n",
    "        os.makedirs(download_dir, exist_ok = True)\n",
    "        \n",
    "        # Create a directory with a unique name\n",
    "        document_id = uuid4().hex\n",
    "        directory = os.path.join(download_dir, document_id)\n",
    "        os.makedirs(directory, exist_ok = False)\n",
    "\n",
    "        # Download the PDF file\n",
    "        if download_url != \"\":\n",
    "            pdf_filename = os.path.join(directory, os.path.basename(download_url))\n",
    "            download_file(download_url, pdf_filename)\n",
    "\n",
    "        # Download the cover image\n",
    "        if cover_image_url != \"\":\n",
    "            cover_image_filename = os.path.join(directory, \"cover_image.jpg\")\n",
    "            download_file(cover_image_url, cover_image_filename)\n",
    "\n",
    "        # Create metadata.json and store relevant details\n",
    "        metadata = {\n",
    "            \"document_id\"       : document_id,\n",
    "            \"title\"             : title,\n",
    "            \"pdf_filename\"      : os.path.basename(pdf_filename),\n",
    "            \"cover_image_url\"   : cover_image_url,\n",
    "            \"pdf_download_url\"  : download_url,\n",
    "            \"overview\"          : overview\n",
    "        }\n",
    "\n",
    "        metadata_file = os.path.join(directory, \"metadata.json\")\n",
    "        with open(metadata_file, \"w\") as f:\n",
    "            json.dump(metadata, f, indent = 4)\n",
    "\n",
    "        status = True\n",
    "    \n",
    "    except Exception as exception:\n",
    "        print(exception)\n",
    "    \n",
    "    finally:\n",
    "        # Stop the webdriver\n",
    "        driver.quit()\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "\n",
    "    csv_file = os.getenv(\"STAGE_1_FILENAME\", None)\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                if len(row) != 2:\n",
    "                    print(f\"Skipping invalid row: {row}\")\n",
    "                    continue \n",
    "\n",
    "                title, url = row\n",
    "                print(f\"Downloading: {title}\")\n",
    "                \n",
    "                # Call download for each title and URL\n",
    "                success = download(title, url)\n",
    "                \n",
    "                if success:\n",
    "                    print(f\"Downloaded: {title}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download: {title}\")\n",
    "\n",
    "    except Exception as exception:\n",
    "        print(\"Error occurred: \", exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"Defined Contribution Plans: Challenges and Opportunities for Plan Sponsors\", \"https://rpc.cfainstitute.org/research/foundation/2021/defined-contribution-plans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"Investor Risk Profiling: An Overview\", \"https://rpc.cfainstitute.org/research/foundation/2015/investor-risk-profiling-an-overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"Lifetime Financial Advice: A Personalized Optimal Multilevel Approach\", \"https://rpc.cfainstitute.org/research/foundation/2024/lifetime-financial-advice-a-personalized-optimal-multilevel-approach\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
